---
title: 'GAISE: Multivariate Appendix'
author: "Nicholas Horton (nhorton@amherst.edu)"
date: "January 15, 2016"
output:
  word_document:
    fig_height: 3
    fig_width: 5
  pdf_document:
    fig_height: 3
    fig_width: 5
  html_document:
    fig_height: 3
    fig_width: 5
---


```{r, include=FALSE}
# Don't delete this chunk if you are using the mosaic package
# This loads the mosaic and dplyr packages
require(mosaic)
```

```{r, include=FALSE}
# Some customization.  You can alter or delete as desired (if you know what you are doing).

# This changes the default colors in lattice plots.
trellis.par.set(theme=theme.mosaic())  

# knitr settings to control how R chunks work.
require(knitr)
opts_chunk$set(
  tidy=FALSE,     # display code as typed
  size="small"    # slightly smaller font for code
)

```{r, include=FALSE}
# Load additional packages here.  Uncomment the line below to use Project MOSAIC data sets.
# require(mosaicData)   
```

### Introduction

The new ASA guidelines for undergraduate programs in statistics recommend that students obtain a clear understan
ding of principles of statistical design and tools to assess and account for the possible impact of other measur
ed and unmeasured confounding variables (ASA, 2014).  (See also Wild's "On locating statistics in the world of finding out", http://arxiv.org/abs/1507.05982.)
An introductory statistics course cannot cover these topics in depth, but it is important to expose students to 
them even in their first course (Meng, 2011).

Perhaps the best place to start is to consider how a third variable can change our understanding of the relationship between two variables. 

In this appendix we describe simple examples where a third factor clouds the association between two other variables.  Simple approaches (such as stratification) can help to discern the true associations. Stratification requires no advanced methods, nor even any inference (though some instructors may incorporate other related concepts and approaches such as multiple regression). These examples can help to introduce students to techniques for assessing relationships between more than two variables.  (The *Journal of Statistics Education* Datasets and Stories department features a number of interesting multivariable examples that feature confounding.)

What is especially concerning in a first (and often only) statistics course is that the simple examples we use to introduce statistics concepts can give students the false impression that all data come from well-conducted randomized trials with no dropout, full adherence, and sufficient blinding and that regression models always include all the appropriate predictors. We hope that introductory statistics courses can open students' eyes to the complexity of real data and the need to take that complexity into account.

Including one or more multivariable examples early in an introductory statistics course may help to prepare students to deal with more than one or two variables at a time.


\section{\textbf{Smoking in Whickham}}


A follow-up study of 1,314 people in Whickham, England characterized smoking
status at baseline, then mortality after 10 years\cite{appl:1996}.  The data are provided in 
Table \ref{tab:whick1}.

\begin{table}[!ht]
\begin{center}
\begin{tabular}{|l|l|l|}
\hline
SMOKER & Alive & Dead \\ \hline
No & 502 (68.6\%) & 230 (31.4\%) \\ \hline
Yes & 443 (76.1\%) & 139 (23.9\%) \\ \hline
\end{tabular}
\caption{Distribution of smoking status and mortality status (after ten years) from the Whickham study}
\label{tab:whick1}
\end{center}
\end{table}

We see that the risk of dying is lower for smokers than for non-smokers, since 
31.4\% of the non-smokers died, but only 23.9\% of the smokers did not survive over the ten year period.  

A graphical representation using a mosaicplot (also known as an \emph{Eikosogram}) represents the cell 
probabilities as a function of area.  

\begin{figure}[bh]
<<whick1,warning=FALSE, fig.height=4.8,message=FALSE,echo=FALSE>>=
require(gmodels); require(mosaic)
Whickham <- mutate(Whickham,
                   agegrp = cut(age, breaks=c(1, 44, 64, 100), labels=c("18-44", "45-64", "65+")))
mosaicplot(tally(~ outcome + smoker, data=Whickham), ylab="Smoking status", 
  xlab="Mortality status (after 10 years)", main="", color=TRUE)
# with(Whickham, CrossTable(smoker, outcome, prop.c=FALSE, prop.chisq=FALSE, prop.t=FALSE))
@
\caption{Mosaicplot representing the association between smoking status and mortality in the Whickham
study}
\label{fig:whick1}
\end{figure}

We note that the majority of subjects have survived, but that
the area for the smokers who are still alive is larger than we would expect if there were no 
association between these variables.
What could explain this result?

Let's consider stratification by age of the participants.  Table \ref{tab:whick2} and Figure \ref{fig:whick2} displays the relationship
between smoking and mortality over a 10--year period for those age 18--44, those 45-64, and subjects that were 65 or older at baseline.
\begin{table}[!ht]
\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline
Baseline age & SMOKER & Alive & Dead \\ \hline \hline
18-44 & No & 327 (96.5\%) & 12 (3.5\%) \\  \hline
18-44 & Yes & 270 (94.7\%) & 15 (5.3\%) \\ \hline \hline
45-64 & No & 147 (73.5\%) & 53 (26.5\%) \\ \hline
45-64 & Yes & 167 (67.6\%) & 80 (32.4\%) \\ \hline \hline
65+ & No & 28 (14.5\%) & 165 (85.5\%) \\ \hline
65+ & Yes & 6 (12.0\%) & 44 (88.0\%) \\ \hline 
\end{tabular}
\caption{Distribution of smoking status and mortality status (after 10 years) stratified by age (at baseline) from the Whickham study}
\label{tab:whick2}
\end{center}
\end{table}

\begin{figure}[bhp]
\includegraphics[width=5.0in]{includes/Whickham.pdf}
\caption{Mosaicplot representing the association between mortality and smoking status (stratified by baseline age) in the Whickham study}
\label{fig:whick2}
\end{figure}

<<whick2,warning=FALSE, message=FALSE,echo=FALSE>>=
require(gmodels); require(mosaic)
#with(filter(Whickham, agegrp=="18-44"), CrossTable(smoker, outcome, prop.c=FALSE, prop.chisq=FALSE, prop.t=FALSE))
@

We see that mortality rates are low for the youngest group, but the mortality rate is slightly higher for smokers than non-smokers (5.3\% for smokers vs 3.5\% for non-smokers).  (Given such low baserates, the information value of the first of the three stratified mosaicplots is low.)

Similar results are seen 
for subjects who are between 45--64 years old at baseline: smokers have a higher
probability of mortality than non-smokers.
\marginnote{Smoking is ``bad" within each of the subgroups of age, while smoking is ``good" overall.}
Almost all of the participants who were 65 or older at baseline died during the followup period, but the 
probability of dying was also slightly higher for smokers than non-smokers.

This example represents a classic example of \emph{Simpson's paradox}\cite{simp:1951,nort:divi:2015}, where 
overall smoking appears to be ``protective", but within each age group smokers have a higher probability
of dying than non-smokers.



How can this be happening?  Figure \ref{fig:whickage1} and Table \ref{tab:whickage1} help
us to disentangle these relationships.  

\begin{figure}[bh]
<<whickage1,warning=FALSE, fig.height=4.8,message=FALSE,echo=FALSE>>=
mosaicplot(tally(~ agegrp + outcome, data=Whickham), ylab="mortality status (after 10 years)", 
  xlab="Age group", main="", color=TRUE)
#with(Whickham, CrossTable(agegrp, smoker, prop.c=FALSE, prop.chisq=FALSE, prop.t=FALSE))
@
\caption{Mosaicplot representing the association between mortality and age in the Whickham
study}
\label{fig:whickage1}
\end{figure}


\begin{table}[htpb]
\begin{center}
\begin{tabular}{|l|l|l|}
\hline
Age group & Alive & Dead \\ \hline
18-44 & 597 (95.7\%) & 27 (4.3\%) \\ \hline
45-64 & 314 (70.2\%) & 133 (29.8\%) \\ \hline
65+ & 34 (14.0\%) & 209 (86.0\%) \\ \hline
\end{tabular}
\caption{Distribution of age group and mortality status (after 10 years) 
from the Whickham study}
\label{tab:whickage1}\end{center}
\end{table}

Not surprisingly, we see that
mortality rates are highest for the oldest subjects.

We also observe that 
there is an association between age group and smoking status, as displayed in 
Figure \ref{fig:whickage2} and Table \ref{tab:whickage2}.

\begin{figure}[bh]
<<whickage2,warning=FALSE, fig.height=3.8,message=FALSE,echo=FALSE>>=
mosaicplot(tally(~ agegrp + smoker, data=Whickham), ylab="smoking status", 
  xlab="Age group", main="", color=TRUE)
@
\caption{Mosaicplot representing the association between smoking status and age in the Whickham
study}
\label{fig:whickage2}
\end{figure}

\begin{table}[htpb]
\begin{center}
\begin{tabular}{|l|l|l|}
\hline
Age group & Non-smoker & Smoker \\ \hline
18-44 & 339 (54.3\%) & 285 (45.7\%) \\ \hline
45-64 & 200 (44.7\%) & 247 (55.3\%) \\ \hline
65+ & 193 (79.4\%) & 50 (20.6\%) \\ \hline
\end{tabular}
\caption{Distribution of age group and smoking status (at baseline) 
from the Whickham study}
\label{tab:whickage2}\end{center}
\end{table}

Smoking is also associated with age, with those between
the ages of 45 to 64 most likely to have been smokers at baseline.  


After controlling for age, smokers have a higher
rate of mortality than non-smokers in this study.
<<whick4,warning=FALSE, message=FALSE,echo=FALSE>>=
#with(filter(Whickham, agegrp=="65+"), CrossTable(smoker, outcome, prop.c=FALSE, prop.chisq=FALSE, prop.t=FALSE))
@


Simple methods such as stratification can allow students to think beyond two 
dimensions and reveal effects of confounding variables.  Introducing this
thought process early on helps students easily transition to analyses involving multiple explanatory variables.





\section{\textbf{SAT scores and teacher salaries}}

Consider an example where statewide data from the mid-1990's are used to assess the association between average teacher salary in the state and average SAT (Scholastic Aptitude Test) scores for students 
\cite{gube:1999} \cite{hort:2015}.  These high stakes high school exams are sometimes used as a proxy for educational quality.  
Figure \ref{fig:sat} displays the (unconditional) association between these variables.  There is a statistically significant negative relationship ($\hat{\beta_1}$=-5.54 points, $p=0.001$). The model predicts that a state with an average salary that is one thousand dollars
higher than another would have SAT scores that are on average 5.54 points lower.

\begin{figure}[bh]
\begin{center}
<<sat1,fig.height=3.5,fig.width=5.8,message=FALSE,echo=FALSE,warning=FALSE>>=
require(mosaic); require(mosaicData)
trellis.par.set(theme=col.mosaic(bw=TRUE)); options(digits=3)
mod1 <- lm(sat ~ salary, data=SAT)

mod2 <- lm(sat ~ salary + frac, data=SAT)

SAT <- transform(SAT, fracgrp = cut(frac, breaks=c(0, 22, 49, 81),
labels=c("low fraction", "medium fraction", "high fraction")))
xyplot(sat ~ salary, type=c("r", "p"),
               xlab="Average teacher salary (in thousands of $)",
               ylab="State average SAT score", data=SAT)
@
\caption{Association of state average teacher salary with average SAT score}
\label{fig:sat}
\end{center}
\end{figure}


But the real story is hidden behind one of the ``other factors" that we warn students about but do not generally teach how to address!  The proportion of students taking the SAT varies dramatically between states, as do teacher salaries.
In the midwest and plains states, where teacher salaries tend to be lower, relatively few high school students take the SAT exam.  
These are
typically the top students who are planning to attend college out of state, while many others take the
alternative standardized ACT test.  For each of the three groups of states defined by the fraction taking the SAT, the association is 
non-negative.  The net result is that the fraction
taking the SAT is a confounding factor.  

This problem is a continuous example of Simpson's paradox.
Statistical thinking with an appreciation of Simpson's paradox would alert a student to \emph{look for} the hidden confounding variables.
To tackle this problem, students need to know that multivariable modeling exists (but not all aspects
of how it can be utilized).

Within an introductory statistics course, the use of stratification by a potential confounder is easy to implement.
By splitting
states up into groups based on the fraction of
students taking the SAT it is possible to account for this confounder and use 
bivariate methods to assess the relationship for each of the groups.

The scatterplot in Figure \ref{fig:sat2} displays a grouping of
states with 0-22\% of students (``low fraction", top line), 23-49\% of students (``medium fraction", middle line), and
50-81\% (``high fraction", bottom line). The story is clear: there is a positive or flat relationship between teacher salary
and SAT score for each of these groups, but when we average over them, we observe a negative relationship.



\begin{figure}[tpbh]
\begin{center}
<<sat2,fig.height=5.2,fig.width=6.7,echo=FALSE,message=FALSE,warning=FALSE>>=
xyplot(sat ~ salary, type=c("r", "p"), groups=fracgrp, auto.key=TRUE, 
       xlab="Average teacher salary (in thousands of $)", 
       ylab="State average SAT score", data=SAT)
@
\caption{Association between teacher salary and SAT score after accounting for the fraction of students taking the SAT in that state}
\label{fig:sat2}
\end{center}
\end{figure}

<<>>=
require(GGally)
ggpairs(select(SAT, salary, sat, frac))
@

Further light is shed via a matrix of scatterplots (see the above Figure): we see that
the fraction of students taking the SAT is negatively associated with the average statewide SAT scores and positively associated with statewide teacher salary.

Recall that in a multiple regression model that controls for the fraction of students taking the SAT variable, the
sign of the slope parameter for teacher salary flips from negative to positive.} 

It's important to have students look for possible confounding factors when the relationship isn't what they expect, but it is also important when the relationship is what is expected.  It's not always possible to stratify by factors (particularly if important confounders are not collected).  

### Multiple regression

The most common multivariable model is a multiple regression. Regression can be introduced as soon as students have seen scatterplots and thought about the patterns we look for in them. When students have access to a statistics program on a computer, they can find regression analyses for themselves. But even without computer access, they can learn about typical regression output tables.
The point is to show students a model involving three (or more) variables and discuss some of the subtleties of such models. Here is one example.

Scottish hill races are scheduled throughout the year and throughout the country of Scotland (http://www.scottishhillracing.co.uk). The official site gives the current records (in seconds) for men and women in these races along with facts about each race including the distance covered (in km) and the total amount of hill climbing (in meters). Naturally, both the distance and the climb affect the record times. So a simple regression to predict time from either one would miss an important aspect of the races.

For example, the simple regression of time versus climb for women's records looks like this:
```
Response variable is:	Women's Record
R squared = 85.2%     R squared (adjusted) = 84.9%
s = 1126  with  70-2 = 68  degrees of freedom 

Variable        Coefficient     SE(Coeff)     t-ratio    P-value
Intercept       320.528         222.2          1.44        0.1537
Climb             1.755           0.088       19.8       < 0.0001
```

We see that the time is greater, on average, by 1.75 seconds per meter of climb. The $R^2$ value of 85.2% assures us that the fit of the model is good with 85.2% of the variance in women's records accounted for by a regression on the climb.

But surely that isn't all there is to these races. Longer races should take more time to run. And although an $R^2$ of 0.852 is good, it does leave almost 15% of the variance unaccounted for. 

Multiple regression models work the same way as simple regression models, but include two or more predictors. Most statistics programs fit multiple regressions in the same way as simple ones; you just need to find the way your program wants you to specify more than one predictor.

Here is the regression with both `Climb` and `Distance` as predictors:
```
Response variable is:	Women's Record
R squared = 97.5%     R squared (adjusted) = 97.4%
s = 468.0  with  70 - 3 = 67  degrees of freedom 
Variable      Coefficient  SE(Coeff)     t-ratio   P-value
Intercept     -497.656     102.8         -4.84     < 0.0001
Distance       387.628      21.45        18.1      < 0.0001
Climb            0.852       0.0621      13.7      < 0.0001
```

This regression model shows both the distance and the climb as predictors and has an $R^2$ of 0.975---a substantial improvement. More interesting, the coefficient of {\tt Climb} has changed from 1.75 to 0.85. That's because in a multiple regression, we interpret each coefficient as the effect of its variable on y after allowing for the effects of the other predictors.

### Closing thoughts


Multivariable thinking is critical to make sense of the observational data around 
us.  
This type of thinking might be introduced in stages:

1. learn to identify observational studies,
2. explain why randomized assignment to treatment improves the situation,
3. learn to be wary of cause-and-effect conclusions from observational studies,
4. learn to consider potential confounding factors and explain why they might be confounding factors,
5. use simple approaches (such as stratification) to address confounding

If students
do not have exposure to simple tools for disentangling complex relationships, they may dismiss statistics as an old-school
discipline only suitable for small sample inference of randomized studies.


Multivariable models are necessary when we want to model many aspects of the world more realistically. The real world is complex and can't be described well by one or two variables. Simple examples are valuable for introducing concepts, but when we don't show students realistic models they are left with the impression that statistics is trivial and not really useful. This report recommends that students be introduced to multivariable thinking: preferably early in the introductory course and not as an afterthought at the end of the course.





### References

ASA (2014).  American Statistical Association Undergraduate Guidelines Workgroup. 2014. "2014 Curriculum Guidelines for Undergraduate Programs in Statistical Science". Alexandria, VA: American Statistical Association, http://www.amstat.org/education/curriculumguidelines.cfm

Meng, X.L. (2011).  Statistics: Your chance for happiness (or misery), *The Harvard Undergraduate Research Journal*, 2(1), http://thurj.org/as/2011/01/1259.

